---
title: "R Notebook"
output: html_notebook
---

In a previous notebook Kate did some extensive experiments with the IRR of the reflections. The second piece for this research, having established the reliability of trained raters in evaluating reflections, is to compare reflections coming from two different sources: the first source is a set of reflections collected by M. Padden (explain a little bit this source). The second set of reflections were collected by K. Whalen (explain this.) The idea is to compare the evaluations between these two sets of reflections to see if grades are higher in one case or the other (recall that the question for the paper is "Did they learn and what?")

The data for this analysis is an excel workbook (name of file) that contains the detailed scores for reflections according to the RLF, and also the totals by category, low level/high level skills, and total for the reflection.

## Preliminaries

We will begin by clearing the workspace.
```{r}
rm(list=ls())
```

Load the packages:
```{r message=FALSE}
library(tidyverse)
```

Read the data file:
```{r}
load("reflection_ratings.RData") 
```

The rows are reflections labelled with the code for the subject and the course. See:
```{r}
head(reflection_ratings)
```

In the table above, `3S 1` is reflection 1 in course 3S.

The columns are the items that are being rated, which are grouped in the following categories:

Category | Code
-|-
TP | Temporal Progression
IA | Important Aspects of the Experience
AT | connection to Academic Theory
ROC | Relating to Other Contexts
TF | Personal Thoughts and Feelings
CE | Cause-and-Effect Relationships
OR | Other Possible Responses
P | Planning and Future Practices

Furthermore, there is a Total Score for each reflection, a REF with the code of the course (`3S03` and `MP`), and a code for the rater (`R`, `M`, `K`). Each rater independently rated each reflection.

## Comparison of means: totals

Compare the total scores between the three raters:
```{r}
ggplot(data = reflection_ratings, 
       aes(x = `Total_Score`, 
           color = Rater)) + 
    geom_freqpoly(bins = 5)
```

Compare the total scores between the two courses:
```{r}
ggplot(data = reflection_ratings, 
       aes(x = `Total_Score`, 
           color = Course)) + 
    geom_freqpoly(bins = 5)
```

What are the mean scores for the three raters?:
```{r}
reflection_ratings %>% 
    filter(Rater == "R") %>% 
    summarize(mean_R = mean(Total_Score))
reflection_ratings %>% 
    filter(Rater == "M") %>% 
    summarize(mean_M = mean(Total_Score))
reflection_ratings %>% 
    filter(Rater == "K") %>% 
    summarize(mean_K = mean(Total_Score))
```

What are the mean scores for the two classes? This is the mean of 3S03:
```{r}
reflection_ratings %>% 
    filter(Course == "3S03") %>% 
    summarize(mean_3S03 = mean(Total_Score))
```

This is the mean of MP:
```{r}
reflection_ratings %>% 
    filter(Course == "MP") %>% 
    summarize(mean_MP = mean(Total_Score))
```

What are the standard deviations of each mean value?
```{r}
reflection_ratings %>% 
    filter(Course == "3S03") %>% 
    summarize(sd_3S03 = sd(Total_Score))
reflection_ratings %>% 
    filter(Course == "MP") %>% 
    summarize(sd_MP = sd(Total_Score))
```

The mean scores are quite different and as expected, the mean score is higher for 3S which is the course where the students received training to complete reflections.

Is this difference significant?

We can test this by implementing a t-test:
```{r}
t.test(x = reflection_ratings %>% 
           filter(Course == "3S03") %>% 
           select(Total_Score), 
       y = reflection_ratings %>% 
           filter(Course == "MP") %>% 
           select(Total_Score), 
       alternative = c("greater"), 
       mu = 0, 
       paired = FALSE, 
       var.equal = FALSE, 
       conf.level = 0.95)
```

Because the p-value is less than 0.005, we will reject the null hyopthesis that the mean reflection scores are the same.

We will continue to compare the mean values for each of the eight RLF components and will also calculate the standard deviation for reporting purposes.

## Comparison of means Temporal Progression

```{r}
reflection_ratings <- reflection_ratings %>%
    mutate(TP_Score = TP_recall + TP_organize)
```

What are the mean scores for the two classes?
```{r}
reflection_ratings %>% 
    filter(Course == "3S03") %>% 
    summarize(mean_TP_3S03 = mean(TP_Score))
reflection_ratings %>% 
    filter(Course == "MP") %>% 
    summarize(mean_TP_MP = mean(TP_Score))
```

The mean scores are not as different. Is this difference significant?

We can test this by implementing a t-test:
```{r}
t.test(x = reflection_ratings %>% 
           filter(Course == "3S03") %>% 
           select(TP_Score), 
       y = reflection_ratings %>% 
           filter(Course == "MP") %>% 
           select(TP_Score), 
       alternative = c("greater"), 
       mu = 0, 
       paired = FALSE, 
       var.equal = FALSE, 
       conf.level = 0.95)
```

Because the p-value is greater than 0.05, we cannot reject the nul. The mean values for Temporal Progression are not significantly different.

## Comparison of means Important Aspects

```{r}
reflection_ratings <- reflection_ratings %>% 
    mutate(IA_Score = IA_differentiate + IA_summarize)
```

What are the mean scores for the two classes?
```{r}
reflection_ratings %>% 
    filter(Course == "3S03") %>% 
    summarize(mean_IA_3S03 = mean(IA_Score))
reflection_ratings %>% 
    filter(Course == "MP") %>% 
    summarize(mean_IA_MP = mean(IA_Score))
```

Is this difference significant?

We can test this by implementing a t-test:
```{r}
t.test(x = reflection_ratings %>% 
           filter(Course == "3S03") %>% 
           select(IA_Score), 
       y = reflection_ratings %>% 
           filter(Course == "MP") %>% 
           select(IA_Score), 
       alternative = c("greater"), 
       mu = 0, 
       paired = FALSE, 
       var.equal = FALSE, 
       conf.level = 0.95)
```

Here we have a high p-value. Thus, we cannot reject the null. The mean values are not significantly different.

## Comparison of means Academic Theory

```{r}
reflection_ratings <- reflection_ratings %>%
    mutate(AT_Score = AT_compare + AT_infer)
```

What are the mean scores for the two classes?
```{r}
reflection_ratings %>% 
    filter(Course == "3S03") %>% 
    summarize(mean_AT_3S03 = mean(AT_Score))
reflection_ratings %>% 
    filter(Course == "MP") %>% 
    summarize(mean_AT_MP = mean(AT_Score))
```

The mean scores are very different. Is this difference significant?

We can test this by implementing a t-test:
```{r}
t.test(x = reflection_ratings %>% 
           filter(Course == "3S03") %>% 
           select(AT_Score), 
       y = reflection_ratings %>% 
           filter(Course == "MP") %>% 
           select(AT_Score), 
       alternative = c("greater"), 
       mu = 0, 
       paired = FALSE, 
       var.equal = FALSE, 
       conf.level = 0.95)
```

Becaue the p-value is less than 0.05, we can reject the null. The mean differences for Academic Theory are significantly different.

## Comparison of means Relating to Other Contexts

```{r}
reflection_ratings <- reflection_ratings %>%
    mutate(ROC_Score = ROC_differentiate + ROC_compare)
```

What are the mean scores for the two classes?
```{r}
reflection_ratings %>% 
    filter(Course == "3S03") %>% 
    summarize(mean_ROC_3S03 = mean(ROC_Score))
reflection_ratings %>% 
    filter(Course == "MP") %>% 
    summarize(mean_ROC_MP = mean(ROC_Score))
```

The mean scores are very different. Is this difference significant?

We can test this by implementing a t-test:
```{r}
t.test(x = reflection_ratings %>% 
           filter(Course == "3S03") %>% 
           select(ROC_Score), 
       y = reflection_ratings %>% 
           filter(Course == "MP") %>% 
           select(ROC_Score), 
       alternative = c("greater"), 
       mu = 0, 
       paired = FALSE, 
       var.equal = FALSE, 
       conf.level = 0.95)
```

Because the p-value is less than 0.05, we can reject the null. The diference in mean values is significant. 

## Comparison of means Thoughts and Feelings

```{r}
reflection_ratings <- reflection_ratings %>%
    mutate(TF_Score = TF_attribute + TF_organize + TF_summarize)
```

What are the mean scores for the two classes?
```{r}
reflection_ratings %>% 
    filter(Course == "3S03") %>% 
    summarize(mean_TF_3S03 = mean(TF_Score))
reflection_ratings %>% 
    filter(Course == "MP") %>% 
    summarize(mean_TF_MP = mean(TF_Score))
```

We can test the difference of mean by means of a t-test:
```{r}
t.test(x = reflection_ratings %>% 
           filter(Course == "3S03") %>% 
           select(TF_Score), 
       y = reflection_ratings %>% 
           filter(Course == "MP") %>% 
           select(TF_Score), 
       alternative = c("greater"), 
       mu = 0, 
       paired = FALSE, 
       var.equal = FALSE, 
       conf.level = 0.95)
```

Because the p-value is less than 0.05, we can reject the null. The means are significantly different. 

## Comparison of means Cause and Effect

```{r}
reflection_ratings <- reflection_ratings %>% 
    mutate(CE_Score = CE_differentiate + CE_explain + CE_critique)
```

What are the mean scores for the two classes?
```{r}
reflection_ratings %>% 
    filter(Course == "3S03") %>% 
    summarize(mean_CE_3S03 = mean(CE_Score))
reflection_ratings %>% 
    filter(Course == "MP") %>% 
    summarize(mean_CE_MP = mean(CE_Score))
```

The means are very close here.

We can test this by implementing a t-test:
```{r}
t.test(x = reflection_ratings %>% 
           filter(Course == "3S03") %>% 
           select(CE_Score), 
       y = reflection_ratings %>% 
           filter(Course == "MP") %>% 
           select(CE_Score), 
       alternative = c("greater"), 
       mu = 0, 
       paired = FALSE, 
       var.equal = FALSE, 
       conf.level = 0.95)
```

Because the p-value is below 0.05, we can reject the null hypothesis. The mean values are significantly different.

## Comparison of means Other Reponses

```{r}
reflection_ratings <- reflection_ratings %>%
    mutate(OR_Score = OR_differentiate + OR_generate + OR_critique)
```

What are the mean scores for the two classes?
```{r}
reflection_ratings %>% 
    filter(Course == "3S03") %>% 
    summarize(mean_OR_3S03 = mean(OR_Score))
reflection_ratings %>% 
    filter(Course == "MP") %>% 
    summarize(mean_OR_MP = mean(OR_Score))
```

The means are different, but are they significant?

We can test this by implementing a t-test:
```{r}
t.test(x = reflection_ratings %>% 
           filter(Course == "3S03") %>% 
           select(OR_Score), 
       y = reflection_ratings %>% 
           filter(Course == "MP") %>% 
           select(OR_Score), 
       alternative = c("greater"), 
       mu = 0, 
       paired = FALSE, 
       var.equal = FALSE, 
       conf.level = 0.95)
```

Because the p-value is less than 0.05, we can reject the null. The mean values are significantly different. 

## Comparison of means Plan

```{r}
reflection_ratings <- reflection_ratings %>% 
    mutate(P_Score = P_generate + P_plan + P_check)
```

What are the mean scores for the two classes?
```{r}
reflection_ratings %>% 
    filter(Course == "3S03") %>% 
    summarize(mean_P_3S03 = mean(P_Score))
reflection_ratings %>% 
    filter(Course == "MP") %>% 
    summarize(mean_P_MP = mean(P_Score))
```

The means are similar.

We can test whether they are different by implementing a t-test:
```{r}
t.test(x = reflection_ratings %>% 
           filter(Course == "3S03") %>% 
           select(P_Score), 
       y = reflection_ratings %>% 
           filter(Course == "MP") %>% 
           select(P_Score), 
       alternative = c("greater"), 
       mu = 0, 
       paired = FALSE, 
       var.equal = FALSE, 
       conf.level = 0.95)
```

Because the p-value is greater than 0.05, we cannot reject the null. The mean values are not significantly different.

## Comparison of means Recount

```{r}
reflection_ratings <- reflection_ratings %>% 
    mutate(Recount_Score = TP_recall + TP_organize + IA_differentiate + IA_summarize + AT_compare + AT_infer)
```

What are the mean scores for the two classes?
```{r}
reflection_ratings %>% 
    filter(Course == "3S03") %>% 
    summarize(mean_Recount_3S03 = mean(Recount_Score))
reflection_ratings %>% 
    filter(Course == "MP") %>% 
    summarize(mean_Recount_MP = mean(Recount_Score))
```

The means are quite different, but are they significantly different?

We can test this by implementing a t-test:
```{r}
t.test(x = reflection_ratings %>% 
           filter(Course == "3S03") %>% 
           select(Recount_Score), 
       y = reflection_ratings %>% 
           filter(Course == "MP") %>% 
           select(Recount_Score), 
       alternative = c("greater"), 
       mu = 0, 
       paired = FALSE, 
       var.equal = FALSE, 
       conf.level = 0.95)
```

Because the p-value is less than 0.05, we can reject the null. The mean values are significatnly different.

## Comparison of means Discuss

```{r}
reflection_ratings <- reflection_ratings %>% 
    mutate(Discuss_Score = ROC_example + ROC_differentiate + ROC_compare + 
               TF_attribute + TF_organize + TF_summarize + 
               CE_differentiate + CE_explain + CE_critique + 
               OR_differentiate + OR_generate + OR_critique + 
               P_generate + P_plan + P_check)
```

What are the mean scores for the two classes?
```{r}
reflection_ratings %>% 
    filter(Course == "3S03") %>% 
    summarize(mean_Discuss_3S03 = mean(Discuss_Score))
reflection_ratings %>% 
    filter(Course == "MP") %>% 
    summarize(mean_Discuss_MP = mean(Discuss_Score))
```

We can test the difference of means by implementing a t-test:
```{r}
t.test(x = reflection_ratings %>% 
           filter(Course == "3S03") %>% 
           select(Discuss_Score), 
       y = reflection_ratings %>% 
           filter(Course == "MP") %>% 
           select(Discuss_Score), 
       alternative = c("greater"), 
       mu = 0, 
       paired = FALSE, 
       var.equal = FALSE, 
       conf.level = 0.95)
```

Because the p-value is less than 0.05, we can reject the null. The mean are significantly different. 
